{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Decoding-Data-Science/airesidency/blob/main/1_evaluation_recipe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a782644a",
      "metadata": {
        "id": "a782644a"
      },
      "source": [
        "### Chatbot And RAG Evaluation\n",
        "\n",
        "Retrieval Augmented Generation (RAG) is a technique that enhances Large Language Models (LLMs) by providing them with relevant external knowledge. It has become one of the most widely used approaches for building LLM applications.\n",
        "\n",
        "This tutorial will show you how to evaluate your RAG applications using LangSmith. You'll learn:\n",
        "\n",
        "1. How to create test datasets\n",
        "2. How to run your RAG application on those datasets\n",
        "3. How to measure your application's performance using different evaluation metrics\n",
        "\n",
        "#### Overview\n",
        "A typical RAG evaluation workflow consists of three main steps:\n",
        "\n",
        "1. Creating a dataset with questions and their expected answers\n",
        "2. Running your RAG application on those questions\n",
        "3. Using evaluators to measure how well your application performed, looking at factors like:\n",
        " - Answer relevance\n",
        " - Answer accuracy\n",
        " - Retrieval quality\n",
        "\n",
        "For this tutorial, we'll create and evaluate a bot that answers questions about a few of Lilian Weng's insightful blog posts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0581b36",
      "metadata": {
        "id": "d0581b36"
      },
      "source": [
        "### Chatbot Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \\\n",
        "  python-dotenv \\\n",
        "  langsmith \\\n",
        "  langchain \\\n",
        "  langchain-openai \\\n",
        "  langchain-community \\\n",
        "  langchain-text-splitters \\\n",
        "  openai \\\n",
        "  pandas \\\n",
        "  tiktoken\n"
      ],
      "metadata": {
        "id": "vcGgC5yK8Xko"
      },
      "id": "vcGgC5yK8Xko",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5b4f5cf5",
      "metadata": {
        "id": "5b4f5cf5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Read secrets from Colabâ€™s User Secrets\n",
        "OPENAI_API_KEY = userdata.get(\"openai\")\n",
        "LANGSMITH_API_KEY = userdata.get(\"LANGSMITH_API_KEY\")\n",
        "\n",
        "if not OPENAI_API_KEY or not LANGSMITH_API_KEY:\n",
        "    raise ValueError(\"Please set OPENAI_API_KEY and LANGSMITH_API_KEY in Colab User Secrets.\")\n",
        "\n",
        "# Set environment variables so the rest of the notebook works unchanged\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = LANGSMITH_API_KEY\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# 1. Install & Imports\n",
        "# =========================\n",
        "!pip install -qU langsmith\n",
        "\n",
        "import os\n",
        "from langsmith import Client\n",
        "\n",
        "# === Set your API key ===\n",
        "# Option A: set directly (for quick demo) â€“ replace with your key\n",
        "# os.environ[\"LANGSMITH_API_KEY\"] = \"YOUR_LANGSMITH_API_KEY\"\n",
        "\n",
        "# Option B (recommended in Colab):\n",
        "# Use \"Secrets\" in Colab (from the left sidebar) and then:\n",
        "# os.environ[\"LANGSMITH_API_KEY\"] = os.environ.get(\"LANGSMITH_API_KEY\")\n",
        "\n",
        "client = Client()\n",
        "\n",
        "# =========================\n",
        "# 2. Create Dataset\n",
        "# =========================\n",
        "dataset_name_new = \"Recipe Bot Evaluation â€” Q/A (Beginner + FDA Set) â€” 6th Dec\"\n",
        "dataset = client.create_dataset(dataset_name_new)\n",
        "\n",
        "# =========================\n",
        "# 3. Original Examples (from your snippet)\n",
        "# =========================\n",
        "original_examples = [\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"How many teaspoons are in one tablespoon?\",\n",
        "            \"context\": \"US kitchen measurement equivalents.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"3\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"What is the safe internal temperature for cooked chicken (Â°C)?\",\n",
        "            \"context\": \"Food safety guideline for poultry doneness.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"74Â°C\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"Convert 2 US cups to milliliters.\",\n",
        "            \"context\": \"Use the US legal cup for home cooking.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"480 ml\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"What is the classic vinaigrette oil-to-acid ratio?\",\n",
        "            \"context\": \"Standard salad dressing ratio.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"3:1\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"Substitute for 1 cup light brown sugar using white sugar and molasses.\",\n",
        "            \"context\": \"Common home-baking substitution.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"1 cup white sugar + 1 tbsp molasses\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"Minimum internal temperature for medium-rare steak (Â°C).\",\n",
        "            \"context\": \"Typical doneness temperature.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"57Â°C\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"How many grams are in 1 ounce (oz)?\",\n",
        "            \"context\": \"Kitchen weight conversion.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"28.35 g\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"What gas is produced when baking soda reacts with an acid?\",\n",
        "            \"context\": \"Leavening reaction in quick breads.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"Carbon dioxide\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"Boiling time for a soft-boiled egg (runny yolk) after simmering starts.\",\n",
        "            \"context\": \"Stovetop method, large eggs.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"6 minutes\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"How many tablespoons are in 1/4 cup (US)?\",\n",
        "            \"context\": \"US kitchen measurement equivalents.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"4 tbsp\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"Q1\",\n",
        "            \"context\": \"US Measurements\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"1 tbsp\"}\n",
        "    }\n",
        "]\n",
        "\n",
        "# =========================\n",
        "# 4. FDA / USDA Facts + Edge Cases\n",
        "# =========================\n",
        "extra_examples = [\n",
        "    # --- Food safety temperatures ---\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"What is the USDA safe internal temperature for ground beef (Â°C)?\",\n",
        "            \"context\": \"US food safety temperature guidelines.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"71Â°C\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"What is the bacterial 'Danger Zone' temperature range (Â°C)?\",\n",
        "            \"context\": \"FDA food safety storage rules.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"5Â°C to 60Â°C\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"How long can cooked food safely sit at room temperature before it should be discarded?\",\n",
        "            \"context\": \"General US food safety rule for perishable foods.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"Maximum 2 hours\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"To what internal temperature (Â°C) should leftovers be reheated for safety?\",\n",
        "            \"context\": \"US food safety guideline for reheating leftovers.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"74Â°C\"}\n",
        "    },\n",
        "\n",
        "    # --- Handling & cross-contamination ---\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"Should raw chicken be washed before cooking?\",\n",
        "            \"context\": \"FDA guidance on cross-contamination in home kitchens.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"No â€” washing raw chicken can spread bacteria through splashing.\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"What is the minimum recommended time for proper handwashing?\",\n",
        "            \"context\": \"Food safety guidance for handwashing.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"At least 20 seconds\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"How long can raw chicken be safely stored in the refrigerator?\",\n",
        "            \"context\": \"USDA guidance for refrigerated storage of raw poultry.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"1 to 2 days\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"At what freezer temperature (Â°C) should food be stored to keep it safe long-term?\",\n",
        "            \"context\": \"US food safety freezer storage guidelines.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"âˆ’18Â°C or lower\"}\n",
        "    },\n",
        "\n",
        "    # --- Allergens ---\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"Name three allergens from the FDA Big Nine list.\",\n",
        "            \"context\": \"US FDA major food allergen list.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"Examples include milk, eggs, and peanuts.\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"Is sesame one of the FDA-recognized major allergens?\",\n",
        "            \"context\": \"FDA Big Nine allergen list, updated in recent years.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"Yes, sesame is one of the major allergens.\"}\n",
        "    },\n",
        "\n",
        "    # --- Measurement conversions & basics ---\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"How many milliliters are in one US tablespoon?\",\n",
        "            \"context\": \"US kitchen measurement standards.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"About 14.79 ml (often rounded to 15 ml).\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"How many fluid ounces are in one US cup?\",\n",
        "            \"context\": \"US kitchen volume measurements.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"8 fluid ounces\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"Approximately how many grams are in 1 cup of all-purpose flour?\",\n",
        "            \"context\": \"Typical baking reference for US recipes.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"Around 120 g, though it can vary with measurement method.\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"How many teaspoons are in 1/2 tablespoon?\",\n",
        "            \"context\": \"US kitchen measurement equivalents.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"1.5 teaspoons\"}\n",
        "    },\n",
        "\n",
        "    # --- Edge case: regional/measurement ambiguity ---\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"Are US and UK pints the same size?\",\n",
        "            \"context\": \"International measurement comparisons that can confuse recipes.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"No â€” a US pint is about 473 ml, while a UK pint is about 568 ml.\"}\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"How many sticks of butter make 1 cup in US recipes?\",\n",
        "            \"context\": \"Common US baking measurement for butter.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"2 sticks of butter equal 1 cup (about 226 g).\"}\n",
        "    },\n",
        "\n",
        "    # --- Edge case: visual checks vs thermometer ---\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"Can you rely on the color of chicken meat alone to know if it is safely cooked?\",\n",
        "            \"context\": \"FDA advice on checking doneness of poultry.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"No â€” color is not reliable; you must check that the internal temperature reaches 74Â°C.\"}\n",
        "    },\n",
        "\n",
        "    # --- Edge case: freezing and bacteria ---\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"Does freezing meat kill harmful bacteria?\",\n",
        "            \"context\": \"Food preservation and safety guidance.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"No â€” freezing usually does not kill bacteria; it mainly stops them from growing.\"}\n",
        "    },\n",
        "\n",
        "    # --- Edge case: unit confusion (Fahrenheit vs Celsius) ---\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"Is 165Â°F the same as 65Â°C for cooked chicken?\",\n",
        "            \"context\": \"Comparing common food safety temperatures between Fahrenheit and Celsius.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"No â€” 165Â°F is about 74Â°C, not 65Â°C.\"}\n",
        "    },\n",
        "\n",
        "    # --- Edge case: flour weight variability ---\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"Does 1 US cup of flour always weigh exactly 120 g?\",\n",
        "            \"context\": \"Baking measurement variability.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"No â€” 120 g is a common reference, but actual weight can range roughly 100â€“130 g depending on how it is measured.\"}\n",
        "    },\n",
        "\n",
        "    # --- Edge case: rare steak & risk groups ---\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"Is rare steak safe for everyone to eat?\",\n",
        "            \"context\": \"Food safety risk levels for different groups of people.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"Rare steak can be acceptable for healthy adults when properly handled, but higher-risk groups like pregnant people, older adults, and immunocompromised individuals are advised to avoid undercooked meat.\"}\n",
        "    },\n",
        "\n",
        "    # --- Edge case: hot holding temperature ---\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"What is the minimum hot-holding temperature (Â°C) recommended for cooked foods?\",\n",
        "            \"context\": \"US food service guidance for keeping cooked food hot and safe.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"About 60Â°C or higher is recommended for hot holding.\"}\n",
        "    },\n",
        "\n",
        "    # --- Edge case: 'room temperature' ambiguity ---\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"Is there a single exact temperature for 'room temperature' in recipes?\",\n",
        "            \"context\": \"Culinary terminology and approximate temperature ranges.\"\n",
        "        },\n",
        "        \"outputs\": {\"answer\": \"No â€” it is not an exact standard; in cooking it usually means around 20â€“22Â°C.\"}\n",
        "    }\n",
        "]\n",
        "\n",
        "# =========================\n",
        "# 5. Upload all examples to LangSmith\n",
        "# =========================\n",
        "client.create_examples(\n",
        "    dataset_id=dataset.id,\n",
        "    examples=original_examples + extra_examples\n",
        ")\n",
        "\n",
        "print(f\"Created dataset: {dataset_name_new}\")\n",
        "print(f\"Total examples uploaded: {len(original_examples) + len(extra_examples)}\")\n"
      ],
      "metadata": {
        "id": "qpUKT6AHEuev"
      },
      "id": "qpUKT6AHEuev",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f20dd601",
      "metadata": {
        "id": "f20dd601"
      },
      "source": [
        "### Define Metrics (LLM As A Judge)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "12e88e52",
      "metadata": {
        "id": "12e88e52"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from langsmith import wrappers\n",
        "\n",
        "openai_client=wrappers.wrap_openai(openai.OpenAI())\n",
        "\n",
        "eval_instructions = \" Strict grader for short recipe Q&A\"\n",
        "\n",
        "def correctness(inputs:dict,outputs:dict, reference_outputs:dict)->bool:\n",
        "      user_content = f\"\"\"You are grading the following question:\n",
        "    {inputs['question']}\n",
        "    Here is the real answer:\n",
        "    {reference_outputs['answer']}\n",
        "    You are grading the following predicted answer:\n",
        "    {outputs['response']}\n",
        "    Respond with CORRECT or INCORRECT:\n",
        "    Grade:\n",
        "    \"\"\"\n",
        "      response=openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            temperature=0,\n",
        "            messages=[\n",
        "                  {\"role\":\"system\",\"content\":eval_instructions},\n",
        "                  {\"role\":\"user\",\"content\":user_content}\n",
        "            ]\n",
        "      ).choices[0].message.content\n",
        "\n",
        "      return response == \"CORRECT\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "e4326a96",
      "metadata": {
        "id": "e4326a96"
      },
      "outputs": [],
      "source": [
        "## Concisions- checks whether the actual output is less than 2x the length of the expected result.\n",
        "\n",
        "def concision(outputs: dict, reference_outputs: dict) -> bool:\n",
        "    return int(len(outputs[\"response\"]) < 2 * len(reference_outputs[\"answer\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7ad76f1",
      "metadata": {
        "id": "e7ad76f1"
      },
      "source": [
        "### Run Evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "a42fd2bd",
      "metadata": {
        "id": "a42fd2bd"
      },
      "outputs": [],
      "source": [
        "default_instructions = \"Respond to the users question in a short, concise manner (one or two word ) IF it is yes/no answer provide some more facts\"\n",
        "\n",
        "def my_app(question: str, model: str = \"gpt-4.1-nano-2025-04-14\", instructions: str = default_instructions) -> str:\n",
        "    return openai_client.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=0,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": instructions},\n",
        "            {\"role\": \"user\", \"content\": question},\n",
        "        ],\n",
        "    ).choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "e171d6aa",
      "metadata": {
        "id": "e171d6aa"
      },
      "outputs": [],
      "source": [
        "### Call my_app for every datapoints\n",
        "def ls_target(inputs: str) -> dict:\n",
        "    return {\"response\": my_app(inputs[\"question\"])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc97aa85",
      "metadata": {
        "id": "dc97aa85"
      },
      "outputs": [],
      "source": [
        "## Run our evaluation\n",
        "experiment_results=client.evaluate(\n",
        "    ls_target, ## Your AI system\n",
        "    data=dataset_name_new,\n",
        "    evaluators=[correctness,concision],\n",
        "    experiment_prefix=\"gpt-4.1-nano-2025-04-14_1\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LD6nDM0vFMyW"
      },
      "id": "LD6nDM0vFMyW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9158ae4a",
      "metadata": {
        "id": "9158ae4a"
      },
      "outputs": [],
      "source": [
        "### Call my_app for every datapoints - change model\n",
        "def ls_target(inputs: str) -> dict:\n",
        "    return {\"response\": my_app(inputs[\"question\"],model=\"gpt-3.5-turbo\")}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2e58d4f",
      "metadata": {
        "id": "c2e58d4f"
      },
      "outputs": [],
      "source": [
        "## Run our evaluation\n",
        "experiment_results=client.evaluate(\n",
        "    ls_target, ## Your AI system\n",
        "    data=dataset_name_new,\n",
        "    evaluators=[correctness,concision],\n",
        "    experiment_prefix=\"gpt-3.5\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ls_target(inputs: str) -> dict:\n",
        "    return {\"response\": my_app(inputs[\"question\"],model=\"gpt-5.1-2025-11-13\")}\n",
        "\n"
      ],
      "metadata": {
        "id": "C8f3WKkCNrOh"
      },
      "id": "C8f3WKkCNrOh",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7882792",
      "metadata": {
        "id": "e7882792"
      },
      "outputs": [],
      "source": [
        "#new dataset\n",
        "experiment_results=client.evaluate(\n",
        "    ls_target, ## Your AI system\n",
        "    data=dataset_name_new,\n",
        "    evaluators=[correctness,concision],\n",
        "    experiment_prefix=\"gpt-5.1-2025-11-13\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#change the llm model name\n",
        "def ls_target(inputs: str) -> dict:\n",
        "    return {\"response\": my_app(inputs[\"question\"],model=\"gpt-5.1-2025-11-13\")}"
      ],
      "metadata": {
        "id": "DLLP03WPyxQq"
      },
      "id": "DLLP03WPyxQq",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#change the prefix\n",
        "experiment_results=client.evaluate(\n",
        "    ls_target, ## Your AI system\n",
        "    data=dataset_name_new,\n",
        "    evaluators=[correctness,concision],\n",
        "    experiment_prefix=\"gpt-4.1-nano-2025-04-14_1\"\n",
        ")"
      ],
      "metadata": {
        "id": "grkWfZ4BFleG"
      },
      "id": "grkWfZ4BFleG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Summary\n",
        "\n",
        "You tested 4 different GPT models using LangSmith.\n",
        "The results show:\n",
        "\n",
        "All models scored very high on correctness (Âµ â‰ˆ 1.00 or 0.85).\n",
        "\n",
        "All models had 0% failure rate (no crashes, no empty outputs).\n",
        "\n",
        "The speed of the models is different: nano is fastest, GPT-5.1 is slower but more accurate.\n",
        "\n",
        "The dataset size used was 34 test examples.\n",
        "\n",
        "This is a simple comparison of model accuracy and response time.\n",
        "\n",
        "Breakdown of Each Column (Beginner Explanation)\n",
        "1. Model Name\n",
        "\n",
        "Examples:\n",
        "\n",
        "gpt-5.1-2025-11-13-e7a64ff3\n",
        "\n",
        "gpt-4.1-nano\n",
        "\n",
        "gpt-3.5\n",
        "\n",
        "This just tells you which model you tested.\n",
        "(People use this to compare price, speed, and accuracy.)\n",
        "\n",
        "2. base 1\n",
        "\n",
        "This simply means it was the base evaluation run (not fine-tuned, no custom version).\n",
        "\n",
        "3. Âµ 0.85 or Âµ 1.00\n",
        "\n",
        "This Âµ (mu) value represents the average score of all test examples.\n",
        "\n",
        "What it means:\n",
        "\n",
        "1.00 â†’ Perfect. The model got every test case right.\n",
        "\n",
        "0.85 â†’ The model got 85% of the test questions correct.\n",
        "\n",
        "0.56 (second Âµ) â†’ This is another metric, often a secondary score, like reasoning clarity.\n",
        "\n",
        "In your results:\n",
        "\n",
        "GPT-5.1 scored 0.85 (meaning 85% accurate)\n",
        "\n",
        "GPT-4.1-nano scored 1.00 (meaning perfect for this dataset)\n",
        "\n",
        "GPT-3.5 scored 1.00\n",
        "\n",
        "This only means:\n",
        "\n",
        "These models were good enough to get all your simple recipe facts correct.\n",
        "\n",
        "ðŸ”¹ Important:\n",
        "These are simple questions (teaspoon conversions, temperatures, basic facts), so even small models score 1.00.\n",
        "\n",
        "In a real FDA or safety context, large models usually outperform nano/small ones, especially on edge cases.\n",
        "\n",
        "4. Time Columns â€” 0.25s, 2.15s, etc.\n",
        "\n",
        "These show:\n",
        "\n",
        "First number: time to respond\n",
        "\n",
        "Second number: full round-trip latency\n",
        "\n",
        "What it means for beginners:\n",
        "\n",
        "Model\tResponse Speed\n",
        "GPT-4.1 nano\tFastest (0.25 sec)\n",
        "GPT-3.5\tFast (0.35 sec)\n",
        "GPT-4.1 nano #2\tFast (0.34 sec)\n",
        "GPT-5.1\tSlowest (0.61 sec)\n",
        "\n",
        "This is normal:\n",
        "\n",
        "Smaller models â†’ faster\n",
        "\n",
        "Bigger models â†’ slower but better at reasoning and edge cases\n",
        "\n",
        "5. 34\n",
        "\n",
        "This indicates the number of examples run in evaluation:\n",
        "â†’ You tested 34 recipe questions.\n",
        "\n",
        "6. Failure Rate â€” 0%\n",
        "\n",
        "Good news:\n",
        "\n",
        "No model failed\n",
        "\n",
        "No empty results, no errors, no timeouts\n",
        "\n",
        "7. Metadata (null values)\n",
        "\n",
        "These JSON blocks:\n",
        "\n",
        "{\"tags\":null,\"dirty\":null,\"branch\":null,\"commit\":null,\"repo_name\":null,\"remote_url\":null,\"author_name\":null,\"commit_time\":null,\"author_email\":null}\n",
        "\n",
        "\n",
        "This is normal.\n",
        "\n",
        "It just means:\n",
        "\n",
        "You did not connect the run to a GitHub repo\n",
        "\n",
        "There is no version tag associated with this evaluation\n",
        "\n",
        "Beginners can ignore this completely.\n",
        "\n",
        "What This Evaluation Means in Simple Words\n",
        "\n",
        "All your models handled the simple recipe dataset extremely well.\n",
        "Even the smallest \"nano\" model achieved a perfect score.\n",
        "\n",
        "This tells us:\n",
        "\n",
        "âœ” Your test questions are easy\n",
        "\n",
        "Conversions, temperatures, food safety basics â€” models already know these.\n",
        "\n",
        "âœ” The real challenge is edge cases, not core facts"
      ],
      "metadata": {
        "id": "_9iURZUqRMxF"
      },
      "id": "_9iURZUqRMxF"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.13.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}