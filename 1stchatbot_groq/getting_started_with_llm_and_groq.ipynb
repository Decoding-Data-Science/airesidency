{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHHVEjfZpGE7EGulNqyND6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Decoding-Data-Science/airesidency/blob/main/1stchatbot_groq/getting_started_with_llm_and_groq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VREm-CkMI8_6",
        "outputId": "5eb5775c-e653-4e52-f552-cbb621c46b7c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/127.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VDwDZRShIa8f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve API keys from Colab's secure storage\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "\n",
        "# Set them as environment variables\n",
        "if groq_api_key:\n",
        "    os.environ[\"GROQ_API_KEY\"] = groq_api_key\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "if groq_api_key:\n",
        "    os.environ[\"GROQ_API_KEY\"] = groq_api_key\n",
        "    print(\"âœ… Key is set. Sending test prompt...\")\n",
        "\n",
        "    # Step 2: Initialize LLM\n",
        "    llm = ChatGroq(model=\"llama-3.3-70b-versatile\")  # or another supported model\n",
        "\n",
        "    # Step 3: Test a simple call\n",
        "    response = llm.invoke(\"Hello, who are you?\")\n",
        "    print(\"ğŸ¤– Response from LLM:\", response)\n",
        "\n",
        "else:\n",
        "    print(\"âŒ GROQ_API_KEY not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XOlwOhPIxEZ",
        "outputId": "6c4f54cb-5048-4818-d15c-785e6b8a82f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Key is set. Sending test prompt...\n",
            "ğŸ¤– Response from LLM: content='Hello! I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 41, 'total_tokens': 66, 'completion_time': 0.090909091, 'prompt_time': 0.001996307, 'queue_time': 0.203669679, 'total_time': 0.092905398}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'finish_reason': 'stop', 'logprobs': None} id='run-0d3e669b-04ae-40ca-8b82-b402f1b4ebaf-0' usage_metadata={'input_tokens': 41, 'output_tokens': 25, 'total_tokens': 66}\n"
          ]
        }
      ]
    }
  ]
}