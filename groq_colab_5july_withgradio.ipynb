{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPd1cx2SySe9z3PPmp1OF+w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Decoding-Data-Science/airesidency/blob/main/groq_colab_5july_withgradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56k-BRqXWFOo",
        "outputId": "a8735ae8-94f1-4b1f-ccfe-a847eb3c0de9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.29.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
            "Downloading groq-0.29.0-py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.29.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymGssEbJi8NG",
        "outputId": "a84338eb-6488-4e55-db21-287e1e1cbb2b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnYZupvAV9U9",
        "outputId": "a1441fa7-9bc3-4462-cbab-c9133f5b7a3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM can refer to several concepts. Here are a few possible meanings:\n",
            "\n",
            "1. **LLM (Lawyer at Law Master)**: LLM is a postgraduate academic degree pursued by law students. It usually requires a bachelor's degree in law and typically involves coursework and research. It's often pursued by those interested in specializing in a particular area of law or by practitioners seeking to enhance their knowledge.\n",
            "\n",
            "2. **Large Language Model**: Large Language Models (LLMs) are a type of artificial intelligence designed to process and understand human language. This model is trained on a massive dataset of text and can generate human-like language. When interacted with, they can provide answers to complex questions, summarize lengthy texts, and even create content such as stories and articles.\n",
            "\n",
            "3. **Logic and Mathematics (or Learning Models)**: In some cases, LLM can refer to the study of logic and mathematics or the process or methods of learning models in AI and machine learning.\n",
            "\n",
            "LLM in the context of AI and large language models is the most common usage today.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "\n",
        "# Define the URL for the Groq API endpoint\n",
        "url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "\n",
        "# Set the headers for the API request\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {groq_api_key}\"\n",
        "}\n",
        "\n",
        "# Define the body for the API request\n",
        "body = {\n",
        "    \"model\": \"llama-3.1-8b-instant\",\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is llm?\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Send a POST request to the Groq API\n",
        "response = requests.post(url, headers=headers, json=body)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    print(response.json()['choices'][0]['message']['content'])\n",
        "else:\n",
        "    print(\"Error:\", response.json())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCnSTr5bmCVQ",
        "outputId": "607235b3-8059-4f82-eedd-2758779586f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.14)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.1)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import gradio as gr\n",
        "\n",
        "from google.colab import userdata  # Secure storage for API keys in Colab\n",
        "\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "\n",
        "# Define the URL for the Groq API endpoint\n",
        "url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "\n",
        "# Set the headers for the API request\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {groq_api_key}\"\n",
        "}\n",
        "\n",
        "# Function to interact with Groq API\n",
        "def chat_with_groq(user_input):\n",
        "    body = {\n",
        "        \"model\": \"llama-3.1-8b-instant\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": user_input}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=body)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()['choices'][0]['message']['content']\n",
        "    else:\n",
        "        return f\"Error: {response.json()}\"\n",
        "\n",
        "# Create Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=chat_with_groq,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask me anything...\"),\n",
        "    outputs=gr.Textbox(),\n",
        "    title=\"DDS Chat with Groq AI (Llama 3.1-8B)\",\n",
        "    description=\"Type your question below and get a response powered by Groq's Llama 3.1-8B model.\"\n",
        ")\n",
        "\n",
        "# Launch Gradio app\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "D627mfvelNaO",
        "outputId": "d4ee95fa-de60-4481-db72-9fa308e7be3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d41f46ab29c7d976c7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d41f46ab29c7d976c7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for hugging face  u need to set env seperately and save a app.py file\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import gradio as gr\n",
        "\n",
        "# Retrieve the API key from the environment variable\n",
        "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "\n",
        "\n",
        "# Define the API endpoint and headers\n",
        "url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "headers = {\"Authorization\": f\"Bearer {groq_api_key}\"}\n",
        "\n",
        "# Function to interact with Groq API\n",
        "def chat_with_groq(user_input):\n",
        "    body = {\n",
        "        \"model\": \"llama-3.1-8b-instant\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": user_input}]\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=body)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()['choices'][0]['message']['content']\n",
        "    else:\n",
        "        return f\"Error: {response.json()}\"\n",
        "\n",
        "# Create Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=chat_with_groq,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask me anything...\"),\n",
        "    outputs=gr.Textbox(),\n",
        "    title=\"Chat with Groq AI (Llama 3.1-8B)\",\n",
        "    description=\"Type your question below and get a response powered by Groq's Llama 3.1-8B model.\"\n",
        ")\n",
        "\n",
        "# Launch Gradio app\n",
        "if __name__ == \"__main__\":\n",
        "    interface.launch()\n",
        "\n"
      ],
      "metadata": {
        "id": "M0o0K7pXnZKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Imports & API setup ─────────────────────────────────────────────────────────\n",
        "import os\n",
        "import requests\n",
        "import gradio as gr\n",
        "from google.colab import userdata      # Secure storage for API keys in Colab\n",
        "\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "headers = {\"Authorization\": f\"Bearer {groq_api_key}\"}\n",
        "\n",
        "# ── Original function (left exactly as you wrote it) ────────────────────────────\n",
        "def chat_with_groq(user_input):                       #  ❗️unchanged\n",
        "    body = {\n",
        "        \"model\": \"llama-3.1-8b-instant\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": user_input}]\n",
        "    }\n",
        "    r = requests.post(url, headers=headers, json=body)\n",
        "    return (r.json()['choices'][0]['message']['content']\n",
        "            if r.status_code == 200 else f\"Error: {r.json()}\")\n",
        "\n",
        "# ── UI helpers ──────────────────────────────────────────────────────────────────\n",
        "CSS = \"\"\"\n",
        "#center-col {max-width: 650px; margin: 0 auto;}\n",
        "#title {text-align:center; font-size:2rem; font-weight:600;}\n",
        "\"\"\"\n",
        "\n",
        "# ── Build the Gradio app ────────────────────────────────────────────────────────\n",
        "with gr.Blocks(css=CSS, title=\"DDS Chat with Groq AI\") as demo:\n",
        "    gr.Markdown(\"<div id='title'>DDS Chat with Groq AI (Llama 3.1-8B)</div>\")\n",
        "    gr.Markdown(\"Type your question, click **Send**, and get an answer powered by Groq’s Llama 3.1-8B.\")\n",
        "\n",
        "    with gr.Column(elem_id=\"center-col\"):\n",
        "        chatbot = gr.Chatbot(height=400, label=\"Conversation\")\n",
        "        user_box = gr.Textbox(placeholder=\"Ask me anything…\", show_label=False, lines=2)\n",
        "        send_btn = gr.Button(\"Send\", variant=\"primary\")\n",
        "\n",
        "    state = gr.State([])   # keeps (user, bot) tuples\n",
        "\n",
        "    def respond(user_msg, history):\n",
        "        bot_reply = chat_with_groq(user_msg)        # ✅ call the original function\n",
        "        history = history + [(user_msg, bot_reply)]\n",
        "        return history, history                     # update chatbot + state\n",
        "\n",
        "    send_btn.click(\n",
        "        fn=respond,\n",
        "        inputs=[user_box, state],\n",
        "        outputs=[chatbot, state],\n",
        "        queue=False\n",
        "    ).then(lambda: \"\", None, user_box, queue=False)  # clear textbox\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "Kk_Yr19qXhmv",
        "outputId": "482c9f8b-944c-416b-9fc4-a2b573e99be6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-5-2044900057.py:33: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(height=400, label=\"Conversation\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b2ba62af23d1d85381.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b2ba62af23d1d85381.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Imports & API setup ─────────────────────────────────────────────────────────\n",
        "import os, requests, gradio as gr\n",
        "from google.colab import userdata         # Secure storage for API keys in Colab\n",
        "\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "headers = {\"Authorization\": f\"Bearer {groq_api_key}\"}\n",
        "\n",
        "# ── Core function (kept intact) ────────────────────────────────────────────────\n",
        "def chat_with_groq(user_input):                            #  ❗️unchanged\n",
        "    body = {\n",
        "        \"model\": \"llama-3.1-8b-instant\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": user_input}]\n",
        "    }\n",
        "    r = requests.post(url, headers=headers, json=body)\n",
        "    return (r.json()['choices'][0]['message']['content']\n",
        "            if r.status_code == 200 else f\"Error: {r.json()}\")\n",
        "\n",
        "# ── UI helpers ─────────────────────────────────────────────────────────────────\n",
        "CSS = \"\"\"\n",
        "#title         {text-align:center; font-size:2rem; font-weight:600;}\n",
        "#left-col      {width: 64%;}\n",
        "#right-col     {width: 35%; padding-left:1rem;}\n",
        "#right-panel   {border:1px solid #e5e5e5; border-radius:8px; padding:1rem;}\n",
        "\"\"\"\n",
        "\n",
        "LLM_QUESTIONS = [\n",
        "    \"What is Retrieval-Augmented Generation (RAG)?\",\n",
        "    \"Explain Chain-of-Thought prompting in simple terms.\",\n",
        "    \"How do I fine-tune an LLM on my own data?\",\n",
        "    \"What are the security risks of LLM applications?\",\n",
        "    \"Compare zero-shot vs few-shot prompting.\",\n",
        "    \"What is the role of vector databases with LLMs?\"\n",
        "]\n",
        "\n",
        "# ── Build the Gradio app ───────────────────────────────────────────────────────\n",
        "with gr.Blocks(css=CSS, title=\"DDS Chat with Groq AI\") as demo:\n",
        "    gr.Markdown(\"<div id='title'>DDS Chat with Groq AI (Llama 3.1-8B)</div>\")\n",
        "    gr.Markdown(\"Ask anything or choose a quick question on the right.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # ── Left column: Chat interface ───────────────────────────────────────\n",
        "        with gr.Column(elem_id=\"left-col\"):\n",
        "            chatbot   = gr.Chatbot(height=450, label=\"Conversation\")\n",
        "            user_box  = gr.Textbox(placeholder=\"Type your question…\",\n",
        "                                   show_label=False, lines=2)\n",
        "            send_btn  = gr.Button(\"Send\", variant=\"primary\")\n",
        "            state     = gr.State([])      # stores conversation tuples\n",
        "\n",
        "        # ── Right column: Quick-question panel ───────────────────────────────\n",
        "        with gr.Column(elem_id=\"right-col\"):\n",
        "            with gr.Box(elem_id=\"right-panel\"):\n",
        "                gr.Markdown(\"**LLM Quick Questions**\")\n",
        "                question_dd = gr.Dropdown(choices=LLM_QUESTIONS,\n",
        "                                          label=\"Select a question\",\n",
        "                                          interactive=True)\n",
        "                gr.Markdown(\n",
        "                    \"Pick a topic and it will populate the input box. \"\n",
        "                    \"Feel free to edit before sending.\"\n",
        "                )\n",
        "\n",
        "    # ── Logic glue ────────────────────────────────────────────────────────────\n",
        "    def respond(user_msg, history):\n",
        "        reply = chat_with_groq(user_msg)          # ✅ call untouched function\n",
        "        history = history + [(user_msg, reply)]\n",
        "        return history, history                   # update chatbot + hidden state\n",
        "\n",
        "    send_btn.click(\n",
        "        fn=respond,\n",
        "        inputs=[user_box, state],\n",
        "        outputs=[chatbot, state],\n",
        "        queue=False\n",
        "    ).then(lambda: \"\", None, user_box, queue=False)   # clear textbox\n",
        "\n",
        "    # Auto-fill textbox when user picks a preset question\n",
        "    question_dd.change(\n",
        "        lambda q: gr.update(value=q),\n",
        "        inputs=question_dd,\n",
        "        outputs=user_box,\n",
        "        queue=False\n",
        "    )\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "BNmZ12R1X_Fn",
        "outputId": "4616b076-cbde-455f-a440-87de71815b7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-6-2207265713.py:44: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot   = gr.Chatbot(height=450, label=\"Conversation\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'gradio' has no attribute 'Box'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-2207265713.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# ── Right column: Quick-question panel ───────────────────────────────\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"right-col\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"right-panel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"**LLM Quick Questions**\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 question_dd = gr.Dropdown(choices=LLM_QUESTIONS,\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'gradio' has no attribute 'Box'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Imports & API setup ─────────────────────────────────────────────────────────\n",
        "import os, requests, gradio as gr\n",
        "from google.colab import userdata          # Secure storage for API keys in Colab\n",
        "\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "url     = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "headers = {\"Authorization\": f\"Bearer {groq_api_key}\"}\n",
        "\n",
        "# ── Core function (unchanged) ──────────────────────────────────────────────────\n",
        "def chat_with_groq(user_input):                          #  ❗️unchanged\n",
        "    body = {\n",
        "        \"model\": \"llama-3.1-8b-instant\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": user_input}]\n",
        "    }\n",
        "    r = requests.post(url, headers=headers, json=body)\n",
        "    return (r.json()['choices'][0]['message']['content']\n",
        "            if r.status_code == 200 else f\"Error: {r.json()}\")\n",
        "\n",
        "# ── UI helpers ─────────────────────────────────────────────────────────────────\n",
        "CSS = \"\"\"\n",
        "#title       {text-align:center; font-size:2rem; font-weight:600;}\n",
        "#left-col    {width:64%;}\n",
        "#right-col   {width:35%; padding-left:1rem;\n",
        "              border:1px solid #e5e5e5; border-radius:8px; padding:1rem;}\n",
        "\"\"\"\n",
        "\n",
        "LLM_QUESTIONS = [\n",
        "    \"What is Retrieval-Augmented Generation (RAG)?\",\n",
        "    \"Explain Chain-of-Thought prompting in simple terms.\",\n",
        "    \"How do I fine-tune an LLM on my own data?\",\n",
        "    \"What are the security risks of LLM applications?\",\n",
        "    \"Compare zero-shot vs few-shot prompting.\",\n",
        "    \"What is the role of vector databases with LLMs?\"\n",
        "]\n",
        "\n",
        "# ── Build the Gradio app ───────────────────────────────────────────────────────\n",
        "with gr.Blocks(css=CSS, title=\"DDS Chat with Groq AI\") as demo:\n",
        "    gr.Markdown(\"<div id='title'>DDS Chat with Groq AI (Llama 3.1-8B)</div>\")\n",
        "    gr.Markdown(\"Ask anything—or pick a quick question on the right.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # ── Left column: chat interface ───────────────────────────────────────\n",
        "        with gr.Column(elem_id=\"left-col\"):\n",
        "            chatbot  = gr.Chatbot(height=450, label=\"Conversation\")\n",
        "            user_box = gr.Textbox(placeholder=\"Type your question…\",\n",
        "                                  show_label=False, lines=2)\n",
        "            send_btn = gr.Button(\"Send\", variant=\"primary\")\n",
        "            state    = gr.State([])   # stores (user, bot) tuples\n",
        "\n",
        "        # ── Right column: quick-question panel ───────────────────────────────\n",
        "        with gr.Column(elem_id=\"right-col\"):\n",
        "            gr.Markdown(\"**LLM Quick Questions**\")\n",
        "            question_dd = gr.Dropdown(choices=LLM_QUESTIONS,\n",
        "                                      label=\"Select a question\",\n",
        "                                      interactive=True)\n",
        "            gr.Markdown(\n",
        "                \"Pick a topic and it will populate the input box. \"\n",
        "                \"Feel free to edit before sending.\"\n",
        "            )\n",
        "\n",
        "    # ── Logic glue ───────────────────────────────────────────────────────────\n",
        "    def respond(user_msg, history):\n",
        "        reply   = chat_with_groq(user_msg)            # ✅ untouched function\n",
        "        history = history + [(user_msg, reply)]\n",
        "        return history, history                       # chatbot + hidden state\n",
        "\n",
        "    send_btn.click(fn=respond,\n",
        "                   inputs=[user_box, state],\n",
        "                   outputs=[chatbot, state],\n",
        "                   queue=False)\\\n",
        "            .then(lambda: \"\", None, user_box, queue=False)  # clear textbox\n",
        "\n",
        "    # Autocomplete textbox when a preset question is chosen\n",
        "    question_dd.change(lambda q: gr.update(value=q),\n",
        "                       inputs=question_dd,\n",
        "                       outputs=user_box,\n",
        "                       queue=False)\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "xIiFhCy1YgyH",
        "outputId": "bd4e919f-7c80-41bb-ae55-9490fe6fb422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-7-3643399675.py:44: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot  = gr.Chatbot(height=450, label=\"Conversation\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8f9308e766600cf27f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8f9308e766600cf27f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Imports & API setup ─────────────────────────────────────────────────────────\n",
        "import os, requests, gradio as gr\n",
        "from google.colab import userdata\n",
        "\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "url, headers = (\n",
        "    \"https://api.groq.com/openai/v1/chat/completions\",\n",
        "    {\"Authorization\": f\"Bearer {groq_api_key}\"}\n",
        ")\n",
        "\n",
        "# ── Original core function (unchanged) ──────────────────────────────────────────\n",
        "def chat_with_groq(user_input):                       # ← still the same\n",
        "    body = {\n",
        "        \"model\": \"llama-3.1-8b-instant\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": user_input}]\n",
        "    }\n",
        "    r = requests.post(url, headers=headers, json=body)\n",
        "    return (r.json()['choices'][0]['message']['content']\n",
        "            if r.status_code == 200 else f\"Error: {r.json()}\")\n",
        "\n",
        "# ── UI helpers ─────────────────────────────────────────────────────────────────\n",
        "CSS = \"\"\"\n",
        "#header-row  {align-items:center; gap:0.75rem;}\n",
        "#logo        {max-width:60px; border-radius:8px;}\n",
        "#title       {font-size:2rem; font-weight:600; margin:0;}\n",
        "#left-col    {width:64%;}\n",
        "#right-col   {width:35%; padding-left:1rem; border:1px solid #e5e5e5;\n",
        "              border-radius:8px; padding:1rem;}\n",
        "\"\"\"\n",
        "\n",
        "LOGO_URL = (\n",
        "    \"https://raw.githubusercontent.com/Decoding-Data-Science/\"\n",
        "    \"airesidency/main/dds_logo.jpg\"\n",
        ")\n",
        "\n",
        "LLM_QUESTIONS = [\n",
        "    \"What is Retrieval-Augmented Generation (RAG)?\",\n",
        "    \"Explain Chain-of-Thought prompting in simple terms.\",\n",
        "    \"How do I fine-tune an LLM on my own data?\",\n",
        "    \"What are the security risks of LLM applications?\",\n",
        "    \"Compare zero-shot vs few-shot prompting.\",\n",
        "    \"What is the role of vector databases with LLMs?\"\n",
        "]\n",
        "\n",
        "# ── Build the Gradio app ───────────────────────────────────────────────────────\n",
        "with gr.Blocks(css=CSS, title=\"DDS Chat with Groq AI\") as demo:\n",
        "    # Header row: logo + title\n",
        "    with gr.Row(elem_id=\"header-row\"):\n",
        "        gr.Image(value=LOGO_URL, elem_id=\"logo\", show_label=False,\n",
        "                 show_download_button=False)\n",
        "        gr.Markdown(\"<div id='title'>DDS Chat with Groq AI (Llama 3.1-8B)</div>\")\n",
        "\n",
        "    gr.Markdown(\"Ask anything—or pick a quick question on the right.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # ── Left column: chat interface ───────────────────────────────────────\n",
        "        with gr.Column(elem_id=\"left-col\"):\n",
        "            chatbot  = gr.Chatbot(height=450, label=\"Conversation\")\n",
        "            user_box = gr.Textbox(placeholder=\"Type your question…\",\n",
        "                                  show_label=False, lines=2)\n",
        "            send_btn = gr.Button(\"Send\", variant=\"primary\")\n",
        "            state    = gr.State([])\n",
        "\n",
        "        # ── Right column: quick-question panel ───────────────────────────────\n",
        "        with gr.Column(elem_id=\"right-col\"):\n",
        "            gr.Markdown(\"**LLM Quick Questions**\")\n",
        "            question_dd = gr.Dropdown(choices=LLM_QUESTIONS,\n",
        "                                      label=\"Select a question\",\n",
        "                                      interactive=True)\n",
        "            gr.Markdown(\n",
        "                \"Pick a topic and it will populate the input box. \"\n",
        "                \"Feel free to edit before sending.\"\n",
        "            )\n",
        "\n",
        "    # ── Logic glue ───────────────────────────────────────────────────────────\n",
        "    def respond(user_msg, history):\n",
        "        reply   = chat_with_groq(user_msg)\n",
        "        history = history + [(user_msg, reply)]\n",
        "        return history, history\n",
        "\n",
        "    send_btn.click(\n",
        "        fn=respond,\n",
        "        inputs=[user_box, state],\n",
        "        outputs=[chatbot, state],\n",
        "        queue=False\n",
        "    ).then(\n",
        "        lambda: \"\", None, user_box, queue=False          # clear textbox\n",
        "    )\n",
        "\n",
        "    question_dd.change(\n",
        "        lambda q: gr.update(value=q),\n",
        "        inputs=question_dd,\n",
        "        outputs=user_box,\n",
        "        queue=False\n",
        "    )\n",
        "\n",
        "demo.launch()\n",
        "\n"
      ],
      "metadata": {
        "id": "AyyZyyX6Ze92",
        "outputId": "5fb5cfbd-9b14-4606-f046-8a2498c9c18a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-8-89524776.py:58: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot  = gr.Chatbot(height=450, label=\"Conversation\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://9c05a900d8bec38a85.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9c05a900d8bec38a85.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}